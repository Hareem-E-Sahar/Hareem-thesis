% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Background}
\label{chp:background}
\textcolor{blue}{TO DO: \\1. Add a section on verification latency or elaborate it in the Just-in-time software defect prediction section. \\2. Add online learning and explain what is an online approach.\\3. Add time travel\\4. Address Abram's comments }
%These can be just explained where they are mentioned already instead of giving them a separate section.
%May be define changes.
%Change classification in Tan Ming's thesis
%See this for verification latency :Just-In-Time Quality Assurance with Assistance of Automated Defect Prediction and Code Testing

\section{Empirical Software Engineering}
Empirical Software Engineering (ESE) is the study of software and its development in an empirical way using the real-world data. Instead of relying purely on theoretical models, ESE draws on the wealth of information stored in software repositories—such as version control histories, issue trackers, discussion forums, and source code—to understand how software evolves, how developers work, and what factors influence software quality, cost, and maintenance.

With the rise of platforms like GitHub and GitLab, there is a vast amount of publicly available software artifacts and rich metadata about the development processes. This includes source code, bug reports, commit messages, developer discussions, and release histories inspiring a shift towards data-driven methods. These artifacts offer valuable insights that can inform the design of developer assistance tools and support evidence-based decision making in software engineering. 


A key venue dedicated to this line of work is the Mining Software Repositories (MSR) conference, which brings together researchers focused on leveraging data from software repositories to tackle real-world problems in development and maintenance. One particularly active area is mining bug repositories. Analyzing bug reports has proven useful for tasks such as bug triaging, fault localization, and predicting future defects. 

%Key goals of ESE include identifying best practices, evaluating the effectiveness and efficiency of development methodologies (e.g., Agile, DevOps), measuring software quality attributes (such as maintainability or testability), and understanding human and organizational factors in software teams. The field often employs both qualitative and quantitative research technique.

Empirical studies in software engineering often adopt well-established research methods such as case studies, surveys, experiments, and observational analyses. These methods help explore questions ranging from how developers collaborate, to what makes code more prone to bugs, to whether certain practices improve productivity or code quality. Broadly, these efforts can be categorized as exploratory (understanding phenomena), explanatory (explaining relationships), or technical validation (evaluating tools and techniques). Together, these methods ensure that conclusions about software processes, methods, and tools are grounded in real-world data and statistically sound analysis, ultimately improving the practice of software development and making software more reliable and maintainable.

\section{Source Code Representations}
Effective source code representation is important for a wide range of software engineering tasks, including defect prediction, clone detection, natural language processing (NLP) on code, information retrieval (IR), and code search and retrieval. The choice of representation impacts the performance, interpretability and computational efficiency of models designed for specific SE tasks. 

\subsection{Lexical and Token-Based Representations}
One common approach to deal with raw source code is to convert it into a sequence of lexical tokens (e.g., keywords, operators, identifiers) using various tokenization methods. Token-based models, such as Bag-of-Words (BoW), Term Frequency Inverse Document Frequency (TF-IDF) or n-gram models, are lightweight and effective for many baseline tasks. They form the basis of traditional IR methods and are still used in defect prediction models~\cite{pornprasit2021jitline} due to their simplicity and computational efficiency. However, token-based representations often ignore the syntactic and semantic aspects of the code, limiting their suitability for tasks involving structural analysis . 

\subsection{Syntactic Representations}
To capture the structure, syntactic representations such as Abstract Syntax Trees (ASTs) are used. ASTs encode the hierarchical structure of programs by representing code as a tree where each node corresponds to a language construct (e.g., method, loop, variable declaration). This structure enables the extraction of rich syntactic features and that combined with hierarchical information makes ASTs particularly effective for tree-based similarity measures, such as tree kernels and subtree matching. Such representations can benefit  tasks like clone detection, where structural similarities indicate potential code duplication or reuse.

\subsection{Semantic Representations}
Semantic representations aim to capture the meaning and behavior of code instead of just its structure. These include symbolic representations (e.g., variable roles, types, function signatures) and graph-based models that encode semantic relationships.  
With the rise of deep learning, learned representations of code, commonly referred to as embeddings, have become pervasive. These embeddings are typically derived from neural language models pre-trained on a large corpora of source code. They capture both syntactic and semantic relationships among code elements. 
Embeddings can be broadly categorized based on the underlying model architecture, including sequence-based models (e.g., RNNs, LSTMs), transformer-based models (e.g., CodeBERT, GraphCodeBERT), and graph-based models (e.g., those using Graph Neural Networks). Code embeddings are particularly useful for bridging vocabulary mismatches in code search by abstracting away from surface-level textual differences to focus on deeper semantic intent. Moreover, they can be fine-tuned for specific downstream tasks and have shown superior performance in applications such as code summarization, clone detection, and code retrieval.

In this thesis, we adopt TF-IDF vector representations motivated by the need to build fast online software defect prediction models. Subsequently, to better capture structural similarities, we leverage ASTs, which naturally reflect source code's hierarchical structure. ASTs enable the application of tree kernels  methods specifically suited for quantifying similarity between trees. %%Additionally, these representations align well with our broader objective of developing explainable software engineering models.

\section{Source Code Similarity}
Finding similarity among software engineering (SE) artifacts—such as historical defects, reused code snippets, or fix patterns—is crucial, as it allows us to effectively leverage past knowledge for critical tasks like software defect prediction, clone detection, and source code retrieval. In SE, similarity between queries and code is essential for code recommendation, while similarity between bug reports and source code aids in bug localization. Similarly, comparing code segments supports clone and plagiarism detection. Central to these tasks are similarity detection algorithms, whose effectiveness depends heavily on the underlying representations, such as textual content, structural information, or semantics.

Textual similarity approaches commonly use Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF operates by calculating Term Frequency (TF), indicating how often a term appears in a specific document, and Inverse Document Frequency (IDF), measuring how rare the term is across all documents. Together, TF highlights frequently occurring terms within individual code segments, while IDF reduces the weight of widely used terms. Cosine distance is then applied to measure similarity between these TF-IDF vectors, quantifying similarity based on the angle between vector representations of different code segments. BM25 algorithm improves upon TF-IDF by measuring term importance differently and introducing parameters to normalize term frequency. It introduces a saturation function for term frequency, ensuring that the impact of a term does not increase linearly with frequency. It further incorporates document length normalization to prevent longer code segments from being unfairly penalized or favored. These adjustments allow BM25 to deliver more stable and accurate similarity scores across documents of varying lengths and content density, making it more robust than TF-IDF in many retrieval settings. Textual approaches have been effectively used for tasks like bug localization but  their retrieval performance may suffer from vocabulary mismatches.


%The BM25 algorithm refines textual similarity by addressing some limitations inherent in TF-IDF. Unlike TF-IDF, which treats each term occurrence linearly, BM25 introduces parameters that normalize term frequency, preventing overly long documents from dominating similarity scores. It incorporates both term saturation and length normalization factors, making similarity calculations more robust, particularly when comparing documents of varying lengths. By accounting for these factors, BM25 typically yields more accurate similarity measurements in practice, enhancing retrieval performance and mitigating the effects of vocabulary mismatches that commonly affect TF-IDF-based approaches.

%Unlike TF-IDF, which treats each term occurrence linearly, BM25 introduces parameters that normalize term frequency, preventing overly long documents from dominating similarity scores. It incorporates both term saturation and length normalization factors, making similarity calculations more robust, particularly when comparing documents of varying lengths. These adjustments allow BM25 to deliver more stable and accurate similarity scores across documents of varying lengths and content density, making it more robust than TF-IDF in many retrieval settings.

Structural similarity approaches address some of these limitations by explicitly accounting for the syntactic structure of source code, which can be represented using ASTs. Structural techniques like tree kernels and tree edit distance enable the comparison of code segments based on their hierarchical, syntactic relationships rather than their surface-level textual similarity. Tree kernels quantify similarity by capturing structural patterns within ASTs, enabling the detection of functionally similar code even when lexical similarity is low. Tree edit distance, on the other hand, measures the minimum number of edit operations (insertions, deletions, or modifications) needed to transform one tree into another. This makes it particularly useful for fine-grained comparison of structural differences in code. Graph based techniques that represent program as PDGs use graph isomorphism and sub-graph matching to find similarities. %All these similarity algorithms are effective at identifying structural duplication or finding recurring patterns.

Recent advancements in this area include neural models and embeddings (or distributed vector representations) which integrate both structural and semantic contexts to overcome limitations inherent to purely textual or structural approaches. While embeddings can be compared using the same distance metrics as TF-IDF vectors—such as cosine or Euclidean distance—their strength lies not in the comparison method itself, but in the rich, distributed representations they produce. Capturing deep semantic relationships between SE artifacts, such as source code changes, enables more meaningful comparisons across diverse and lexically varied codebases which makes embeddings very popular in the software engineering. 


Despite their advantages, embeddings come with limitations. They do not explicitly model hierarchical structures, which are often essential for understanding source code changes. Additionally, embeddings are inherently opaque: the individual dimensions in an embedding vector lack clear, interpretable meaning. This lack of explainability poses challenges for tasks such as software defect prediction 
where developers require transparent, explainable insights into why a particular code segment is flagged as defective. Since embeddings are dense, non-interpretable vectors, they do not provide clear reasoning or traceable features behind prediction, making it difficult for developers to validate or trust the results. Additionally, embeddings do not explicitly capture hierarchy inherent in source code such as the relationships between code elements, functions, control flow, which are often crucial for accurately identifying defects. Together, these limitations make embeddings unsuitable for use cases where explainability and structural understanding are essential. Therefore, in this thesis, instead of relying on the default go-to method—embeddings—we explore an alternative route that emphasizes simpler, yet theoretically well-established approaches. Among these methods is TF-IDF and tree kernels, with the help of which we build explainable software defect prediction models while benefiting from the hierarchical structure of source code.


\subsection{Tree kernels for code similarity} 
Tree kernels is a method of comparing trees and computing similarity between tree-structured data without explicitly enumerating all possible subtrees. By comparing parts of trees and finding shared sub-parts, tree kernels can help in identifying recurring structural patterns in data. For example, in NLP, tree kernels are used to compare the syntactic trees of different sentences, identifying noun or verb phrases. In bioinformatics, they are used to compare phylogenetic trees and identify shared evolutionary patterns.
%recurring substructures are nouns and verbs
Similarly, if you have a set of ASTs representing syntactic structures of source code, tree kernels can identify syntactic patterns e.g., loops, method calls, that recur across different code snippets.

 
Instead of flattening code into sequences of tokens or lines, tree kernels retain its hierarchical structure, enabling more fine-grained and syntactically aware similarity measures. This is especially useful for tasks like defect prediction, where risky code structures may be encoded in specific subtrees, or clone detection, where semantically similar but syntactically varied code fragments can still share structural components. 

Tree kernels work by implicitly mapping trees into a high-dimensional feature space defined by the presence of various subtrees. The kernel value is then computed as the inner product in this feature space. This approach avoids the need to manually define features and retains more structural information than bag-of-words models. Tree kernels are also more interpretable than sequential models because one can visualize and inspect matching subtrees whereas dimensions are abstract and hard to explain.

Despite their interpretability and structural richness, tree kernels are computationally expensive and less scalable to large datasets, which limits their adoption in practice. However, their theoretical grounding and ability to capture syntactic patterns make them valuable for research tasks that require structural fidelity which motivated us to evaluate them in this thesis.


%%The similarity between two trees can be measured by comparing their sub-parts.
%%Convolution kernel measures the similarity of two objects in terms of the similarities of their subparts. Convolution kernels are based on counting the number of shared substructures, partially discarding information about their position in the original structure [2]. A subtree rooted at a node N tree must contain all of that node's descendants down to the leave. A subset tree is a substructure where, for any node, either all of its children or none of them are included in the subset tree. Unlike subtrees, the leaves of a subset tree can be internal nodes of the original tree.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\subsection{Clone detection for code similarity}\textcolor{red}{Remove or extend?}
%Two identical fragments of code which are identical or highly similar are called clones. The clones are divided into four types [41]; Type-1 are
%identical code fragments that only differ by whitespaces or comments. Type-2 are syntactically identical code fragments
%that differ by identifier names, data types or literal values. Type-3 are fragments that have been modified due to the
%addition or deletion of statements. Type-4 code fragments are called semantic clone because they semantically similar but share little syntactic similarity. 
%\textcolor{red}{This section is incomplete}


\section{SZZ Algorithm}
To predict future software bugs effectively, we first need historical data about known bugs. Such data is often stored in Version Control Systems and can be systematically extracted. One rich source of this information is the collection of source code changes made to a project. When developers push these changes from their local machines to the central repository, each set becomes a commit, identified by a unique commit hash. Alongside the code, developers typically include a commit message to describe the intent or purpose of the change.

When fixing bugs, developers often reference the related issue number or describe the bug fix directly in their commit messages. This allows us to identify bug-fixing commits. Once a bug-fixing commit is recognized, we can trace back through the project's history to find the commit that introduced the bug. This is exactly what the SZZ algorithm~\cite{szz2005}--proposed in 2005 by Śliwerski, Zimmermann, and Zeller--aims to do.

SZZ starts by identifying all modified lines in a bug-fixing commit using the diff utility. More specifically, it focuses on the lines that were deleted during the fix. It then scans the version history to locate the commit where those deleted lines were originally introduced. If this earlier commit predates the report of the bug, it is marked as a bug-inducing commit.

The fundamental assumption behind SZZ is that bugs are introduced in the same lines that are later modified to fix them. However, subsequent studies have highlighted the limitations of this approach. Not all bugs stem from a direct code change, and SZZ can misidentify bug origins due to noise—such as non-code changes (e.g., comments, renames) or broad refactoring. For example, Neto et al. found that nearly 19.9\% of lines deleted during bug fixes were due to refactorings, making their associated bug-inducing commits false positives.

To address these shortcomings, several SZZ variants have emerged. AG-SZZ leverages annotation graphs to filter out non-semantic changes like blank lines and indentation. MA-SZZ targets noise introduced by meta-changes such as merges and branches. RA-SZZ focuses on removing the effects of refactorings. Among these, RA-SZZ has been shown to produce the cleanest and most accurate results. 


SZZ and its variants have been instrumental in producing datasets for software defect prediction research. Similarly, the technical debt dataset is another prominent example created through an SZZ-based implementation. Despite, its uses there are several limitations that need to be addressed e.g. what about bugs that become known after a latency period, what about one line bugs. It is imperative for the software engineering research community to come up with better datasets to advance defect prediction research.

%One underexplored limitation of the SZZ algorithm is its assumption that each bug fix maps directly to a single inducing change, overlooking the possibility that some bugs emerge from the interaction of multiple commits over time. Such interaction effects—where individually harmless changes introduce faults only when combined—are difficult for SZZ to capture, as it operates on line-level annotations in isolation. This makes SZZ inherently limited in capturing the systemic or emergent nature of certain defects.

%Additionally, SZZ does not differentiate between semantic and non-semantic code changes. It treats any prior change to a line later fixed as potentially fault-inducing, even if that change had no impact on program behavior (e.g., comment edits or code reordering that preserves logic). This reliance on syntactic traceability without semantic understanding leads to false positives and reduces the reliability of fault attribution

A key limitation of the SZZ algorithm is its dependence on explicitly labeled bug-fix commits, typically identified through references to issue trackers or keywords in commit messages. This biases the algorithm toward capturing high-visibility defects—those that are severe enough to be reported, tracked, and fixed through formal channels. However, many low-severity, silent, or developer-noticed-only bugs never make it to the issue tracker and may be fixed opportunistically during unrelated work. These fixes often lack explicit labeling and are thus invisible to SZZ, resulting in an incomplete and potentially skewed view of fault-inducing changes.

\section{Information Retrieval} 
\textcolor{blue}{TO DO:\\
Add BM25 formula\\
Move to Code as Natural Language\\
Explain the shift toward viewing code as natural language and using NLP techniques.\\
Introduce Neural Models\\
Present newer models like CodeBERT, CC2Vec, and neural IR methods.\\
Connect to Your Work\\
End with how your work builds on this foundation (using TF-IDF for code similarity).}

Information Retrieval (IR) is the process of finding relevant information from a collection of unstructured documents in response to user queries. Over the years, classical IR models—such as the Vector Space Model and probabilistic models—have been successfully applied to natural language documents, including software artifacts like bug reports. One widely used technique is TF-IDF, which calculates how important a word is to a document relative to a larger corpus. It gives more weight to words that appear frequently in a document but are rare across the collection, thereby highlighting meaningful terms and filtering out generic ones.

TF-IDF has been applied to various software engineering tasks, such as code search, bug localization, and crash report de-duplication. In code search, for example, developers typically write queries in natural language—often English—to find relevant code snippets. Since TF-IDF relies on matching shared vocabulary between the query and documents, it performs well when the same words are used in both. However, a common challenge in practice is the vocabulary mismatch: developers often describe code behavior using different words than those found in the code itself. To address this, researchers have explored query expansion with synonyms. For example, Lu et al.~\cite{lu2015query} used synonyms from WordNet, a lexical database of English, to enrich queries and then matched them with source code identifiers. FaCOY~\cite{facoy} extended this concept by leveraging Q\&A platforms like Stack Overflow to generate alternate queries, aiding in the identification of semantic code clones.

In the task of bug localization, IR is used to generate a ranked list of potentially defective files to inspect, given a bug report. For instance, Locus~\cite{wen2016locus} measures relevance between bug reports and software changes, and successfully locates bug-inducing changes within the top 5 results for 41.0\% of bugs. While effective, such models still require high inspection effort from developers.

Since TF-IDF represents each document as a vector of weighted terms, cosine similarity is commonly used to compute document similarity. Hindle et al.~\cite{hindle2019query} applied this method to detect duplicate bug reports in real time by querying issue tracking systems prior to submission—helping reduce redundancy in bug databases.

BM25~\cite{BM25}, another classical IR model, improves upon TF-IDF by incorporating document length normalization and saturation effects for term frequency. This makes it more robust in certain scenarios. However, like TF-IDF, BM25 still relies on exact or partial term matching and lacks an understanding of deeper context or semantics.

These limitations have led to a shift toward embedding-based models facilitated by deep learning. Such models capture semantic meaning by learning richer representations of text and code and storing them as dense vectors. Notably, CodeBERT~\cite{feng2020codebert} and CC2Vec~\cite{hoang2020cc2vec} have shown strong performance in a range of IR tasks by going beyond surface-level keyword matching. Pushing semantic retrieval further, Gu et al.~\cite{gu2018deep} introduced CODEnn, which embeds both code snippets and their textual descriptions into the same vector space, enabling meaningful semantic comparisons. Similarly, Lancer~\cite{lancer} combined a library-sensitive language model with BERT to better capture code intent, further improving retrieval effectiveness.

Inspired by these advances, this thesis applies BM25 to a new IR task: detecting similarities between source code changes. By treating code changes as documents, this approach leverages document similarity to detect semantically similar changes. Despite its simplicity, this method serves as a lightweight and explainable approach for just-in-time defect prediction in software projects.

\section{Explainability in Software Engineering}
Software developers have to make critical decisions—debugging, code maintenance, and feature development— at each step of building the software to ensure high quality useful software. As a software engineering researcher, my goal is to ensure that developer assistance tools to support decision making are not only accurate but also explainable. In fact, transparency and trust is extremely important for the adoption of any real world software engineering tool. A developer not only wants to know the cause of a bug, or the reason behind a recommended code change but they also want to understand the reason behind such predictions. Prior work~\cite{jiarpakdee2020empirical} has argued that a lack of explainability of defect prediction models could hinder the adoption of defect prediction models in practice. Therefore, explainability is becoming a central focus in software engineering research, especially in domains like defect prediction, code recommendation, and test case prioritization. If an approach can explain why a file is buggy, why a line was blamed, or why a code change is recommended, it will be called explainable. 

Traditional tools like FindBugs, now SpotBugs\footnote{https://spotbugs.github.io/}~\cite{hovemeyer2004finding} and PMD\footnote{https://pmd.github.io/} use rule-based analysis to identify bugs, code smells or code vulnerabilities. They produce a high number of false positives overwhelming the developers with warnings, many of which are not actually problematic~\cite{johnson2013don}. The violations pointed to by these tools can also lack contextual insights. As a result, developers may distrust or ignore their output, reducing their practical utility during debugging or maintenance tasks. 
Machine learning based approaches, by contrast, are often more accurate but operate as black boxes, and do not inherently explain the reason behind their decisions. This leaves the developers in the dark about the reasoning behind predictions. A real challenge therefore is to design models that have predictive power as well as some explainability.

In my thesis, I address this gap by designing an explainable approach for Just-in-Time software defect prediction. Unlike prior work that focused primarily on predicting bugs, I focus on the explainability aspect to explain why a particular commit was predicted as buggy. I also identified the top most defective lines of a suspected defective commit. Jiarpakdee et al.~\cite{jiarpakdee2021practitioners} surveyed practitioners and found that few defect prediction studies address explainability, despite its importance for developer adoption.

Explainability in ML can be classified into two broad types: global and local. Global explanations aim to summarize the overall behavior of a model across all inputs by relating the independent variables with outcome. For example, a variable importance analysis in decision trees can expose its full structure and allows users to see how decisions are made in the input space. Similarly, techniques like variable importance (e.g., in Random Forests) provide insight into which features are generally influential. While useful for understanding model logic at a high level, global explanations do not explain individual predictions. Local or instance explanations, in contrast, is an explanation of the decision of a black-box model for a particular testing instance. They can be used to explain the reason behind the categorization of a specific instance such as a file, a commit, or a line as defective. These explanations are more relevant for developers looking to understand or act on specific outcomes.

\subsection{Model-Agnostic Local Explanation Techniques}
Local explanations can be generated using model-agnostic techniques like LIME, SHAP. which approximate the complex model locally by replacing it with a simpler, interpretable model—such as a linear model or decision tree. To build a local model, LIME (Local Interpretable Model-agnostic Explanations) generates ``synthetic neighbors" around the instance to be explained---a process called perturbing--- and observing its effect on the model's prediction. For example, it may slightly modify the code tokens or metric values of the input instance to generate synthetic instances. LIME then trains a simple, interpretable model (e.g., a sparse linear regression classifier) on the newly synthesized instances to approximate the complex model's behavior in the neighborhood of the input. The resulting coefficients of the model serve as explanations highlighting which features had the greatest influence on that specific prediction.
SHAP (SHapley Additive exPlanations), on the other hand, is grounded in game theory and computes the contribution of each feature to the prediction based on Shapley values. Unlike LIME, SHAP provides consistent and theoretically justified explanations, though it is often more computationally intensive.
Both methods have been successfully applied in software engineering. For example, Jiarpakdee et al.~\cite{jiarpakdee2021practitioners} found LIME effective in explaining file-level defect predictions, while Pornprasit et al.~\cite{pornprasit2021jitline} used it to identify risky lines (i.e., those containing risky tokens) within buggy commits. To further improve the use of explanations in defect prediction, Pornprasit et al.~\cite{pornprasit2021pyexplainer} proposed PyExplainer, a rule-based, model-agnostic technique that generates human-readable, actionable insights. PyExplainer produces synthetic neighbors that are more similar leading to more accurate local models. As a result, its explanations were found to be more unique and consistent than that of LIME for JIT defect prediction models.

In my thesis, I built upon these ideas to design an explainable approach for Just-in-Time software defect prediction. Unlike prior work that focused primarily on predicting bugs, I focus on the explainability aspect to explain why a particular commit was predicted as buggy. However, I determine the similarity of a commit to a previous risky commit and suggest those prior risky commits as an explanation of the bug. Furthermore, my proposed approach identifies the top most defective lines of an identified buggy commit. This makes the model’s predictions not only more transparent but also more useful for debugging, triaging, and long-term software maintenance.

%It builds a local sparse linear regression model (K-Lasso) whose coefficients indicate the importance score of each feature on the prediction for a specific instance.

\section{Software Defect Prediction}
A software defect—commonly referred to as a bug, fault, or error—is a deviation from the expected behavior. Defects cost the US economy an estimated \$2.41 trillion losses in $2022$\footnote{https://www.it-cisq.org/the-cost-of-poor-quality-software-in-the-us-a-2022-report/}. To prevent these defects and improve software quality, organizations regularly conduct testing and code inspections. However, these activities can be costly and resource-intensive~\cite{thongtanunam2015investigating}, making it impractical to thoroughly examine every part of a large system. 

Software Defect Prediction (SDP) addresses this challenge by identifying software modules (files, classes, or methods) that are most likely to contain defects. By prioritizing these high-risk areas, developers can allocate testing and quality assurance efforts more efficiently. SDP typically aligns with the project's release cycle--using historical snapshots and defect data from past releases to predict which modules are likely to be defective in future versions. These predictions are made at varying levels of granularity, such as packages, files, classes, or methods, and help developers focus their quality assurance efforts to risky parts before their deployment.


SDP is generally formulated as a supervised learning problem, where models are trained using historical code metrics (e.g., lines of code, McCabe's cyclomatic complexity, coupling between objects)~\cite{ck1994}\cite{martin1994} or process metrics (e.g., code churn, number of revisions, number of bug-fixing commits, developer activity)~\cite{nagappan2005}. To evaluate these models, researchers have relied on publicly available defect datasets such as the NASA MDP~\cite{gray2012reflections} and Jureczko datasets~\cite{Jureckzo}, comparing a variety of classifiers including Decision Trees, Naive Bayes, Logistic Regression, Bayesian Networks, and Support Vector Machines~\cite{lessmann2008bench}. A major limitation of these metrics-based models is their high false positives~\cite{clever} which can lead to wasted developer effort when inspecting modules that are ultimately non-defective. 

To address this limitation, later approaches introduced a post-processing step to confirm or refute the predictions made by metric-based models. For instance, Nayrolles et al.,~\cite{clever} proposed a two phase framework called Clever which uses code clone detection to compare predicted buggy components against historical buggy patterns. Similarly, Yan et al.,~\cite{yan2020JIT} adopted a related methodology, but instead used n-gram representations to model buggy code patterns and validate initial predictions.

Despite extensive research, the field of SDP faces several practical challenges, with class imbalance being prominent. Defect datasets are imbalanced in nature, containing very few buggy modules than the clean ones, which negatively impacts the performance of predictive models. This issue has motivated the exploration of balancing techniques\cite{tantithamthavorn2018impact} and ensemble methods\cite{laradji2015software} as strategies to deal with the skewed nature of data. One widely adopted method is Synthetic Minority Over-sampling Technique (SMOTE)~\cite{chawla2002smote}, which balances datasets by generating synthetic examples of the minority class through interpolation. This enables classifiers to better learn from limited but critical data, such as defective modules. 

Another pressing concern in SDP is the lack of explainability~\cite{dam2018explainable}\cite{ribeiro2016should} of models. Techniques like SMOTE, while effective for data balancing, can obscure the relationship between a prediction and the original data, making it difficult to establish traceability. More broadly, machine learning and deep learning models used in defect prediction are often opaque. In particular, models that rely on learned representations, such as embeddings from deep architectures, are difficult to interpret—developers typically cannot understand what individual dimensions represent or why a particular prediction was made. This lack of transparency reduces trust, and ultimately hinders the practical adoption of defect prediction tools in software engineering workflows.

\section{Fine-grained SDP}
Either a separate section or merge into SDP section above. 


\subsection{Validation Settings in of Software Defect Prediction}
Most early studies focus on Within-Project Defect Prediction (WPDP)~\cite{turhan2009relative}\cite{basili1996}, where models are trained and tested on different versions or modules of the same project. WPDP assumes the availability of sufficient historical data within the project and typically yields better predictive performance due to the consistent feature distributions.
However, in many real-world projects—especially new or small ones—labeled defect data may be scarce or unavailable. To address this, researchers have explored Cross-Project Defect Prediction (CPDP), where models are trained on data from other projects and then applied to the target project~\cite{zimmermann2009cross}~\cite{he2012investigation}. While CPDP improves data availability and generalization potential, it introduces significant challenges such as distribution mismatch between source and target projects, making accurate prediction more difficult. To mitigate these issues, various CPDP techniques have been proposed, including data standardization, instance selection~\cite{menzies2013better}, feature mapping, and transfer learning~\cite{nam2013transfer} approaches. Despite their challenges, CPDP models are essential for improving defect prediction applicability in industrial settings lacking historical defect labels.

%On the other hand, the key premise of CPDP is to learn from data from a set of projects and then to apply resulting models to another set of projects [82], [84].

\subsection{Just-in-time software defect prediction} 
Just-in-time software defect prediction (JIT SDP) a.k.a., change-level defect prediction is a way of predicting defects at the commit-level or change-level. It helps determine whether each incremental software change is defective or not, allowing developers to preemptively detect and fix risky changes before they make it to the software repository. JIT defect prediction operates at a finer granularity than release-level prediction, enabling more localized inspections. Since each change typically has a single author, triaging is simpler, and issues are addressed while design decisions are still fresh in developers' minds. Early approaches~\cite{mockus2000predicting} predicted defective changes using a set of code change properties (lines added, lines deleted, code entropy) that were primarily derived from the changes themselves. Follow up works~\cite{kim2008classifying}\cite{kamei2012large} added new properties from the Version Control System (VCS) and the Issue Tracking System (ITS) and code review databases~\cite{kononenko2015investigating} to improve the prediction accuracy of JIT models.
%ITS contains two types of data: natural language text (e.g., issue reports, issue discussions, and code reviews) and meta-data (e.g., the type of issues and the issue creation time). Prior JIT-SDP studies design and investigate features mostly capturing the characteristics of the ITS data other than actual textual contents [72, 74].

%sKamei et al.,~\cite{mcintosh2017fix} introduced the concept of 

Cabral et al.~\cite{cabral2019class} argued that JIT SDP models are  evaluated unrealistically, ignoring the verification latency--- the fact that labeled training examples often arrive with delay which can affect the performance of JIT SDP models. They also highlighted that the class imbalance (i.e., how rare the defect-inducing changes are) may evolve over time. Their work introduced a more realistic evaluation framework that better reflects practical constraints and lays the groundwork for deployable JIT defect prediction models. 
Motivated by these insights, we designed our models for incremental updates, explicitly accounting for verification latency during training.  


\section{Validation Methodologies in Software Engineering and Time Travel} 
Hindle et al. propose that there are three main contexts of evaluation: time agnostic validation, continuous training, and historical splits validation. The time agnostic validation does not regard time and is unrealistic because in real world software artifacts appears at a particular time and in a specific order during software evolution. This context enables cross-fold validation and due to a lack of empirical data in some use cases researchers pragmatically rely on this method of evaluation. This type of evaluation poses a fundamental threat to the validity of results because we are relying on data that does not exist and hence time traveling to build a model. Rakha et al. refer to such an evaluation as ``classical evaluation" and claim that it unrealistically boosts performance by 17\% to 42\%.

\section{Online learning/online JIT-SDP}
\textcolor{red}{Add time travel as it motivates the first paper. Is this the section where I add it?}
Online learning is a way to learn from new data without retraining from scratch. 
IRJIT is an incremental because there is no learning when we use information retrieval. %Decay factor and sliding window was not used.

% these should not be included in background because its discussed in the conclusion stability paper.
%Continuous training. 
%Historical splits validation


\end{document}
