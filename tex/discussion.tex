% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

%\onlyinsubfile{\zexternaldocument*{\main/tex/introduction}}
\begin{document}

\chapter{Discussion}
\label{chp:discussion}
%Todo:
%Add some trees, show common subtrees and motivate that tree kernels are explainable ?\\
%Read "Automatically classifying source code using tree-based approaches".
%~/UofA2025/Literature_Review/Trees_ASTs_and_Graphs/Automatically classifying source code using tree-based approaches
%Read all old notes and find out what discussions were done and include those.

Software defect prediction approaches have not seen widespread adoption in industry. Although researchers acknowledge this gap, there is a notable lack of studies that systematically examine the limitations and boundaries of existing methods that hinder their use in real-world settings. This thesis aims to bring some of these issues to the forefront so that they can be addressed by the research community. In particular, the thesis contributes an assessment of the lack of realism, especially with respect to time, in the evaluation and execution of SDP models.

\section{How my thesis advances SDP}	
%How these contributions pave the way for the adoption of SDP in practice?
%
%Temporal instability in cross-project DP shows that static train/test splits are insufficient.
%
%Reported performance rankings are not stable over time, undermining generalizability claims.
%We can't evaluate on one time period and assume the model hold same performance across time. Models that win in one time slice may lose later. Reported gains don't persist.
%When underlying data changes, performance changes. Evaluations where model was evaluated on a particular point in time, and assuming conclusions generalize beyond those points leads to models that break in production. Time-aware instability explains why many SDP models fail to transfer to practice.
%Evaluation realism is not optional—it directly affects the validity of research conclusions. Avoid single-split, time-agnostic evaluations, rather do longitudinal or incremental or online evaluation.
% 
%
%Researchers need to evaluate across multiple points on the timeline. Couch claims of conclusions to context, and time. 
%
%
%In practice, repeatedly re-training a model is expensive and quickly becomes impractical. Design approaches that do not re-train from scratch are better. IRJIT updates incrementally and demonstrates that online, incremental evaluation is both feasible and informative. Focus on online approaches as JIT is inherently online problem. 
%
%More complex models are not necessarily more useful models.
%Design simpler approaches as those might be as good as the complicated models and might be more interpretable and  easier to explain.
%
%
%Traditionally the focus of SDP  is mostly on maximizing performance. Models optimized for benchmark performance often conflict with constraints faced in real development workflows.
%Practitioners need actionable, lightweight, and explainable models.
%
%IRJIT shows that simple IR-based methods can operate online with minimal assumptions.
%
%Tree kernels evaluate whether richer syntactic structure improves retrieval effectiveness.
%Incorporate structure when determining code similarity because pure lexical matches are not very meaningful. 
%Tree kernels, while expressive, raise concerns about computational cost vs. practical benefit in real deployment scenarios.
%Expensive models should follow up a cheap quick step.
%
%My thesis outlines a few critical issues in SDP research but there might be many more. I point out that the SDP research is questionable and should be re-evaluated.  
%Taken together, this thesis demonstrates that many accepted conclusions in software defect prediction are artifacts of unrealistic evaluation and execution assumptions. By introducing time-aware, incrementally updatable models, this work reframes how SDP models should be assessed, compared, and ultimately deployed.
%%%Industrial validation is limited, but realism is improved relative to prior work.


The techniques presented in this thesis are connected by a single concern: the mismatch between how defect prediction models are evaluated in research and how they would need to operate in practice. Each contribution addresses a different manifestation of this mismatch, progressing from validity of evaluation methods, to model design, to representation choices.

The starting point of this discussion is the observation that temporal instability undermines many existing defect prediction claims. Empirical findings in Chapter~\ref{chp:emse2020} show that k-fold cross-validation yields performance results that are statistically significantly different, though with small effect sizes, compared to evaluations that respect the temporal order of data.
Similarly, the performance and the rankings obtained on a static train-test split are not stable over time. Figure~\ref{fig:stepConfigCC} through Figure~\ref{fig:stepConfigII} show that a model that outperforms others in one time slice may lose its advantage in a later evaluation. Reported gains do not persist, and conclusions drawn at one point in time cannot be assumed to generalize into the future.

Several studies attribute this instability to the software's evolution and the natural shift in the data distribution changes, leading to concept drift. As a result, model behavior changes as well. Evaluations that assume stationary nature of data create optimistic estimates that break under real deployment conditions.
In my opinion, this is a plausible explanation for why many defect prediction models fail to transfer from research settings into production. 

My thesis emphasizes why evaluation realism is not optional; it directly affects the validity of research conclusions and impacts the adoption of research in real world settings. As a result, this thesis argues that defect prediction models should be evaluated across multiple points on the timeline (as we did in ~\ref{chp:irjit}), and that claims of effectiveness must be explicitly contextualized by time of evaluation and data used.

%We also did online approach where models are updated instead of evaluating on one period 
This insight motivated the second contribution of my thesis. If model performance is unstable and evaluation must be incremental, then models themselves must be able to operate incrementally. In practice, repeatedly retraining complex ML or DL models such as JITLine and JITFine from scratch is expensive and quickly becomes impractical. JIT SDP is inherently an online problem, yet many existing approaches are trained offline and evaluated in s settings.
The proposed approach in Contribution 2 (Chapter~\ref{chp:irjit}) of my thesis directly responds to this gap. It demonstrates that an online, incrementally updatable defect prediction model is feasible. By relying on IR rather than training heavy ML and DL pipelines, IRJIT updates naturally as new commits arrive and avoids costly retraining. IRJIT was found to be competitive with state-of-the-art ML and DL approaches, JITLine and JITFine, suggesting that more complex models are not necessarily better performing models. My thesis shows that a simpler approach achieves comparable performance while better aligning with the constraints of real development workflows.

The use of IR was a design choice influenced by prior work of Campbell et al~\cite{campbell2016unreasonable}, and it allows me to address another barrier to adoption of SDP models: lack of interpretability. Traditional SDP research largely prioritizes maximizing benchmark performance. However, models optimized solely for performance may not comply with practitioner needs. Developers require predictions that are explainable and ideally actionable. Decisions obtained from opaque models reduce trust and discourage adoption. By exposing similar historical commits and enabling line-level risk ranking, this contribution of my thesis offers a form of explanation grounded in observable evidence rather than black box model internals.

The third and final contribution of my thesis addresses one limitation of IRJIT: reliance on purely lexical tokens. Pure lexical similarity is often insufficient to find structurally similar snippets, as it ignores program structure that may be relevant to bug introduction. In this contribution, I examine whether richer structure based code representations can further improve SDP performance. Tree kernels provide a theoretically appealing way to capture syntactic structure through abstract syntax trees, and evaluating them allowed us to test whether structural expressiveness translates into practical benefit. 

The results shown in Chapter~\ref{chp:treekernels} show that tree kernels can capture structural similarity in code, providing a richer representation than purely lexical approaches. However, their computational cost is high due to quadratic comparison complexity which constrains their use. 
Due to the philosophy of realism adopted in this thesis, the high computational cost of tree kernels presents a challenge that had to be carefully considered. While structural information is valuable, expensive models must be justified by clear empirical gains.

%A reasonable deployment strategy is therefore hierarchical: inexpensive, fast methods first, followed by more expressive techniques only when warranted
Therefore, in the third contribution of this thesis, I propose a hybrid approach that first applies the inexpensive retrieval mechanism from IRJIT, followed by tree kernels. 
Theoretically, this strategy was a reasonable candidate for empirical evaluation, as it reflects the thesis's emphasis on practicality and scalability in model design. 

Taken together, these contributions suggest that several widely accepted conclusions in software defect prediction are artifacts of unrealistic evaluation and execution assumptions. This thesis does not claim that existing SDP research is invalid, but rather that such conclusions may be questionable and provide an incomplete picture. By introducing time-aware evaluation, incrementally updatable models, and evaluating structural representations, this work reframes how defect prediction models should be evaluated, and designed for realistic settings.


%%By introducing time-aware evaluation, incrementally updatable models, and evaluating structural representations, this work provides evidence that challenges how defect prediction models are currently assessed and motivates more realistic evaluation and design choices.
%%SDP research needs to account for operational constraints, not just accuracy.

%\section{Choice of IR and Tree Kernels instead of ML and DL}
%Most studies use ML, we use IR. Why? How does IR benefit us in designing a realistic approach?
%Light weight, up-to-date/online models, fast to update models, explain-ability, granular predictions.


\subsection{Design Choices and Limitations}
% how to extend explainability and how to evaluate it

%Did we use the right metrics to evaluate IRJIT}

%1. Lexical models are not aware of the contextual semantic information. code frequencies do not care about code semantics. IRJIT overlooks token semantics e.g. it does not understand that 'for' is similar to 'while'.
%2. High FPs for some projects.
%3. We did not evaluate its usefulness with real developers.
%
%
%
%How IRJIT is explainable ?
%
%Consider two illustrative instances: in one, the model solely yields a binary label denoting the presence of vulnerabilities in a code snippet, while in the other, it provides a label accompanied by relevant code snippets showcasing similar vulnerabilities. This latter approach facilitates a deeper understanding of the model’s decision through additional explanation. 
%
%In my thesis, industrial validation was not done and none of the proposed methods were evaluated in an industrial setting neither the approaches were deployed, but realism is substantially improved relative to prior work. The thesis thus outlines a path toward SDP methods that are not only empirically sound, but also credible candidates for adoption in real software development environments.
%
%
%In our research also, we have evaluated the explainability of the state-of-the-art models, but our approach has  limitations, such as the claims of explainability that are not empirically evaluated. We assumed transparent models like IRJIT are explainable, but it does not mean that they are actionable? There is no evaluation to find out if these models provide enough information for end-users to act upon and trust the outputs? Are all transparent models equal in terms of practitioners' ability to understand them? There is enormous potential for future research on the relationship between explainability and the usefulness to practitioners.
%
%
%
%Explainability includes post hoc explanations [116], offering supplemental insights to further clarify the model's decisions. However, researchers have identified inconsistency among explanations provided by different techniques, highlighting the need for more reliable methods.
%
%Another issue is that there is no standard way, benchmark datasetes, or metrics to measure explainability of SDP models.
%Providing fine-grained line-level, interpretable defect predictions increases practical usefulness and improve developer trust but we have not evaluated these claims.
%
%
%In future, we might consider measuring explanation satisfaction, which considers how well the user understands the explanation presented by the model [91]. Variable importance, for example, may not be sufficient for practitioners to understand the model. We could engineer better and more meaningful features for these models, including the potential causes, types, risks or impacts of a bug as variables in the machine-learning models.
%


As an information retrieval based approach, IRJIT relies on lexical similarity and token frequency. While this choice enables designing lightweight, online models, it also means that the model is insensitive to deeper syntactic and semantic relationships in code. IRJIT builds a vocabulary incrementally as it sees new code. Anything it has not seen is out-of-vocabulary and therefore does not contribute to the predictions, which weakens its generalization beyond the exact tokens it has seen.

For example, IRJIT does not capture the functional similarity between constructs such as \textsc{for} and \textsc{while}, nor does it reason about control-flow or behavioral equivalence. As a result, some semantically similar changes may be treated as unrelated, potentially limiting prediction quality in settings where semantic variation dominates over lexical similarity.

Moreover, the token similarity based hints can be coincidental contributing to the high false positive rates as observed for some projects in Table~\ref{table:stats}. This behavior reflects a broader trade-off inherent in retrieval-based approaches: prioritizing recall and simplicity can come at the cost of precision in certain contexts. While this thesis reports these outcomes transparently, it does not propose mechanisms to systematically reduce false positives. %without sacrificing the model's online and incremental nature. 
Addressing this issues remains an open problem for future work.

Another limitation of my thesis is the lack of its industrial validation. The hypothesis of this thesis were not evaluated in an actual software development setting nor were the techniques deployed in real-world development environments, nor were they evaluated through user studies involving actual developers. Consequently, claims about practical usefulness, or adoption are necessarily indirect. That said, the thesis improves realism relative to much prior SDP research by emphasizing time-aware evaluation, incremental model updates, and execution constraints that more closely reflect operational settings. These contributions outline a path toward practice-oriented SDP methods, but do not in themselves demonstrate industrial uptake.

As for the explainability of IRJIT, this thesis adopts reasonable assumptions but stops short of empirical evaluation. IRJIT is explainable in the sense that it provides explanations in the form of documents that support the predictions such as similar past commits, and source code lines ranked by riskiness. While this thesis does not empirically evaluate whether such explanations are useful for the developers or contain sufficient information for them to act upon. Arguably, this additional context plausibly enables a better understanding of model behavior compared to models that output only a binary label, or rely on post-hoc explanations which have been shown to produce inconsistent or even contradictory insights~\cite{roy2022don}. 

That being said, it is imperative to validate such assumptions with the developers as transparency does not necessarily imply usefulness, and not all transparent models are equally understandable in practice. A major barrier in the evaluation of explainability of approaches is that it requires a lot of resources to conduct user studies. Moreover, there is no standard methodology, evaluation benchmarks, or dataset, and set of metrics for measuring explainability in defect prediction models.  that there are cu
More broadly, the evaluation of explainability itself remains underdeveloped in SDP research. There are currently no standard approaches, benchmark datasets, or metrics for measuring explainability in defect prediction models.  While this thesis discusses explainability and its potential approaches, it does not resolve these foundational measurement challenges. 

\subsection{Tree Kernels as an Alternative to Embeddings}

Tree kernels and embeddings offer two different approaches to capturing code similarity, each with distinct trade-offs. Tree kernels work directly on ASTs, which are a natural representation of source code and are understandable by humans, unlike embeddings, which are hard to visualize.
In the context of SDP, tree kernels offer the attractiveness of a simple non-neural alternative, i.e., they require no training or GPU resources. They compute similarity online without consuming significant memory beyond what is needed to store the trees themselves. Consistent with this thesis's focus on simple, lightweight methods applicable in real-world settings with limited offline training budgets, these characteristics motivated the evaluation of tree kernels.
In addition to that, the deterministic nature of tree kernels ensures they will always produce the same similarity scores, given the same two ASTs.

Embeddings, on the other hand, are derived from deep learning models and need significant offline training to produce vector representations. Once the training is done, similarity comparisons are fast. Effectively, they shift the computational cost from inference to the training phase. This shift is problematic in evolving systems characterized by changes in data distributions, concept drift, and verification latency. Without retraining, their representations may become misaligned with the latest code changes, which can lead to degraded performance in realistic, continuously evolving development environments. The use of embeddings or similar models therefore does not align with the thesis's goal of realistic incremental model development.


Neither alternative is strictly superior; rather, they reflect different priorities. Tree kernels offer a training-free, deterministic similarity that is immediately applicable to new data. In the context of this thesis, tree kernels provided an underexplored, practical alternative to embedding-based similarity. While they seemed to perfectly align with the broader focus of our thesis on simple, incrementally usable models that can be updated under realistic constraints, but their high computational complexity and slow nature in our experiments from ~\ref{chp:treekernels} showed that they were of limited practical value in SDP. In contrast, embeddings are not computed on the fly and almost all studies use pre-computed embeddings, which moves their training overhead offline to ensure fast runtime comparisons but tree kernels remain slow at the inference time.  This motivated the hybrid design in the third contribution where IRJIT's fast retrieval was combined with tree kernel re-ranking, to empirically assess the benefits of structural richness while maintaining practical efficiency.



\subsection{Lessons from Tree Kernels}


\subsection{The problem of SDP datasets} 
\textcolor{blue} Abram said that's not very relevant
This can briefly be discussed in threats to validity section as it is not a limitation of my work but SDP in general.

The only defects that get noticed are the ones that make it to the issue tracker and that's what SDP addresses.

SZZ and its variants do not do a great job at labeling. Explainability datasets and metrics are not available.


Why did tree kernels not perform as expected. They should have been evaluated in a supervised manner.
Tree kernels can be used for program repair?

We operate on the operational data.
We are not totally aware of all the issues and bugs.
We operate on the data in the wild.
No instrumented product where we are aware of all issues.
No solely benchmark tool.
Make evaluation benchmark.

%See A Reflection on Change Classification in the Era of Large Language Model

%The repair datasets might give a perspective.

%Daniel Alencar da Costa's survey has some great pointers on which studies offer explainability.

%(see /home/hareem/ThinkpadP14/UofA2023/Tree_Kernel2024/NotesWinter2024.md)
 
 
\section{Do LLMs make SDP irrelevant?}
%Is SDP still needed now that LLM's have taken over?
%probabilistic inference not deterministic .
Have defects increased or decreased with the emergence of LLMs? Does defect distribution change after LLMs? Does the nature and type of defects change? There are papers that have evaluated if LLMs increase or decrease bugs. \\
 
Did the defect distribution change?
Clarify if your data has syntax errors? Report the taxonomy of bugs in historical data and in LLM code?

LLMs are great at producing syntax free code. However, commits are usually syntax free and they contain logical or semantic bugs that LLMs may overlook. Therefore, SDP models are still relevant today.
Software projects often have unique, evolving errors specific to individual code repositories.  
 
Software is part of all domains, including healthcare and security-critical, where the consequences  of bugs are more severe. This has increased the need of reliable code even more. Clankers are producing code at a very fast rate so the need to code review has increased as well. Even code written by LLMs can be bug prone and people care about the riskiness of commits. Due to these reasons the need of SDP models is high but SDP's effectiveness has not improved. With the amount of code we are producing that needs review, the slow models may not remain very useful. Models need to adapt fast. A model trained a year ago may not hold its performance as underlying assumptions could change. Especially with LLMs the old models become obsolete fast.
 
While we may use LLMs to predict bug proneness of code but they may produce many false positives. Therefore, SDP in combination with LLMs might be a much reliable option.
 
Therefore, reliable defect predictions models are still of importance. They should scale to the large amounts of code without being too slow.
 
 
SDP as a field has not kept up with the development pace of SE where LLMs have taken over.
The whole SDP research is questionable and it needs to be re-evaluated. We need to establish strong datasets, better baselines, and an agreed upon evaluation criteria.
 
%LLMs have reasoning ability so we can produce reasonable estimates of code coming in.
%See A reflection on change classification in the era of LLMs.
 
LLMs may also get affected by time-Based Output Drift. Furthermore, there is no assurance that the results will remain consistent over time.
 
AI can automatically generate tests for new code, even without history.
Running these tests gives immediate signals: fail → likely buggy, pass → likely okay.
This could augment SDP models, especially in early stages.
 
LLM-generated tests are not perfect. They might miss edge cases, misunderstand the code, or give false positives/negatives. Traditional SDP can combine multiple sources of information (history, code metrics, developer patterns) to make more reliable predictions. In practice, the best approach is hybrid: use AI-generated test outputs as additional features for SDP, rather than replacing it entirely.

Unlike LLMs, our approach does not rely on training a model with hundreds of billion parameters or generalizing across datasets. It directly analyses the raw data from repositories (code and its structure), making it inherently reproducible and free from probabilistic biases. LLMs are not deterministic.

Their resource requirements are significantly higher, both in terms of model size, training time and computational power. 
There is differences in the cost of repairing buggy Copilot-generated code compared to human-generated code.

A study found that although Copilot improves productivity, it often produces code with insecure patterns. (An Empirical Cybersecurity Evaluation of GitHub Copilot’s Code Contributions)

Several studies point out that LLMs negatively impact software development from a security perspective.
Our study reveals that at least 62.07\% of the LLM generated programs have security vulnerabilities. 
%"How secure is AI-generated code: a large-scale comparison of large language models"

Kla found four types of errors in ChatGPT-generated code: compilation and runtime errors, wrong output (linked to assertion errors), code style and maintainability, and performance and efficiency. ("Refining chatgpt-generated code: characterizing and mitigating code quality issues")

There are no specific studies comparing types or distribution of bugs before and after the adoption of LLMs.

No studies on assessing the effectiveness of traditional SDP models (that are trained solely on human-written code) on the software systems where part of the code is written by AI.


LLMs Speeds up development.
LLM generated code can be syntactically correct but semantically buggy. Developers still need to review carefully.

%"Bugs in large language models generated code: an empirical study"
10 distinctive bug patterns bug patterns were found in code generated by LLMs. These include Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, IncompFacilitatinglete Generation, and Non-Prompted Consideration. 

%"What is wrong with your code generated by large language models? An extensive study"
This paper developed a taxonomy of bugs for LLM code that includes three categories (functional, syntax and runtime bugs) and ten sub-categories, and analyzed the root cause for common bug types. They found that functional bugs and misunderstanding and logic errors are the highest percentage of bugs produced by LLMs.
\end{document}

%We only use code but other sources of information generated during software development may be just as important as the code.


%Bugs negatively impact a system particularly in safety and security-critical domains like autonomous driving (Chen et al., 2015) and healthcare (Cai et al., 2014), the consequences of these bugs can be more severe. 

%Parse tree kernels have been successfully used in natural language processing (NLP). However, the parse tree kernel used in NLP does not perform well for program source code due to two issues. The first issue is the asymmetric influence of node changes. In previous tree kernels, changes near a root node have larger influence than changes near leaf nodes. When the tree depth is small, this effect is not serious. However, the parse trees of program sources are much larger than that of natural language sentences, and this unwanted influence greatly affects tree comparison. The second issue is the sequence of subtrees. Previous tree kernels count sequence of subtrees. Unlike natural language sentences, the sequence of two substructures (like the order of two methods in a Java class) has little information in program source codes. We identified these two issues, and propose a new parse tree kernel for program source code.


%When processing ASTs structures, we usually face a problem of high-dimensional data. The size of an AST increases more than linearly when the program size is larger. In the experimental dataset, the largest AST contains 7027 nodes, while the program has only 343 lines of code including comments and blank lines. High-dimensional data not only lead to waste of time and memory but also aﬀect the performance of algorithms. Thus, dimension reduction is an essential task for the AST-based approach.
