% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

%\onlyinsubfile{\zexternaldocument*{\main/tex/introduction}}
\begin{document}

\chapter{Discussion}
\label{chp:discussion}
%Todo:
%Add some trees, show common subtrees and motivate that tree kernels are explainable ?\\
%Read "Automatically classifying source code using tree-based approaches".
%~/UofA2025/Literature_Review/Trees_ASTs_and_Graphs/Automatically classifying source code using tree-based approaches
%Read all old notes and find out what discussions were done and include those.

Software defect prediction approaches have not seen widespread adoption in industry. Although researchers acknowledge this gap, there is a notable lack of studies that systematically examine the limitations and boundaries of existing methods that hinder their use in real-world settings. This thesis aims to bring some of these issues to the forefront so that they can be addressed by the research community. In particular, the thesis contributes an assessment of the lack of realism, especially with respect to time, in the evaluation and execution of SDP models.

\section{How my thesis advances SDP}	
%How these contributions pave the way for the adoption of SDP in practice?
%
%Temporal instability in cross-project DP shows that static train/test splits are insufficient.
%
%Reported performance rankings are not stable over time, undermining generalizability claims.
%We can't evaluate on one time period and assume the model hold same performance across time. Models that win in one time slice may lose later. Reported gains don't persist.
%When underlying data changes, performance changes. Evaluations where model was evaluated on a particular point in time, and assuming conclusions generalize beyond those points leads to models that break in production. Time-aware instability explains why many SDP models fail to transfer to practice.
%Evaluation realism is not optional—it directly affects the validity of research conclusions. Avoid single-split, time-agnostic evaluations, rather do longitudinal or incremental or online evaluation.
% 
%
%Researchers need to evaluate across multiple points on the timeline. Couch claims of conclusions to context, and time. 
%
%
%In practice, repeatedly re-training a model is expensive and quickly becomes impractical. Design approaches that do not re-train from scratch are better. IRJIT updates incrementally and demonstrates that online, incremental evaluation is both feasible and informative. Focus on online approaches as JIT is inherently online problem. 
%
%More complex models are not necessarily more useful models.
%Design simpler approaches as those might be as good as the complicated models and might be more interpretable and  easier to explain.
%
%
%Traditionally the focus of SDP  is mostly on maximizing performance. Models optimized for benchmark performance often conflict with constraints faced in real development workflows.
%Practitioners need actionable, lightweight, and explainable models.
%
%IRJIT shows that simple IR-based methods can operate online with minimal assumptions.
%
%Tree kernels evaluate whether richer syntactic structure improves retrieval effectiveness.
%Incorporate structure when determining code similarity because pure lexical matches are not very meaningful. 
%Tree kernels, while expressive, raise concerns about computational cost vs. practical benefit in real deployment scenarios.
%Expensive models should follow up a cheap quick step.
%
%My thesis outlines a few critical issues in SDP research but there might be many more. I point out that the SDP research is questionable and should be re-evaluated.  
%Taken together, this thesis demonstrates that many accepted conclusions in software defect prediction are artifacts of unrealistic evaluation and execution assumptions. By introducing time-aware, incrementally updatable models, this work reframes how SDP models should be assessed, compared, and ultimately deployed.
%%%Industrial validation is limited, but realism is improved relative to prior work.


The techniques presented in this thesis are connected by a single concern: the mismatch between how defect prediction models are evaluated in research and how they would need to operate in practice. Each contribution addresses a different manifestation of this mismatch, progressing from validity of evaluation methods, to model design, to representation choices.

The starting point of this discussion is the observation that temporal instability undermines many existing defect prediction claims. Empirical findings in Chapter~\ref{chp:emse2020} show that k-fold cross-validation yields performance results that are statistically significantly different, though with small effect sizes, compared to evaluations that respect the temporal order of data.
Similarly, the performance and the rankings obtained on a static train-test split are not stable over time. Figure~\ref{fig:stepConfigCC} through Figure~\ref{fig:stepConfigII} show that a model that outperforms others in one time slice may lose its advantage in a later evaluation. Reported gains do not persist, and conclusions drawn at one point in time cannot be assumed to generalize into the future.

Several studies attribute this instability to the software's evolution and the natural shift in the data distribution changes, leading to concept drift. As a result, model behavior changes as well. Evaluations that assume stationary nature of data create optimistic estimates that break under real deployment conditions.
In my opinion, this is a plausible explanation for why many defect prediction models fail to transfer from research settings into production. 

My thesis emphasizes why evaluation realism is not optional; it directly affects the validity of research conclusions and impacts the adoption of research in real world settings. As a result, this thesis argues that defect prediction models should be evaluated across multiple points on the timeline (as we did in ~\ref{chp:irjit}), and that claims of effectiveness must be explicitly contextualized by time of evaluation and data used.

%We also did online approach where models are updated instead of evaluating on one period 
This insight motivated the second contribution of my thesis. If model performance is unstable and evaluation must be incremental, then models themselves must be able to operate incrementally. In practice, repeatedly retraining complex ML or DL models such as JITLine and JITFine from scratch is expensive and quickly becomes impractical. JIT SDP is inherently an online problem, yet many existing approaches are trained offline and evaluated in s settings.
The proposed approach in Contribution 2 (Chapter~\ref{chp:irjit}) of my thesis directly responds to this gap. It demonstrates that an online, incrementally updatable defect prediction model is feasible. By relying on IR rather than training heavy ML and DL pipelines, IRJIT updates naturally as new commits arrive and avoids costly retraining. IRJIT was found to be competitive with state-of-the-art ML and DL approaches, JITLine and JITFine, suggesting that more complex models are not necessarily better performing models. My thesis shows that a simpler approach achieves comparable performance while better aligning with the constraints of real development workflows.

The use of IR was a design choice influenced by prior work of Campbell et al~\cite{campbell2016unreasonable}, and it allows me to address another barrier to adoption of SDP models: lack of interpretability. Traditional SDP research largely prioritizes maximizing benchmark performance. However, models optimized solely for performance may not comply with practitioner needs. Developers require predictions that are explainable and ideally actionable. Decisions obtained from opaque models reduce trust and discourage adoption. By exposing similar historical commits and enabling line-level risk ranking, this contribution of my thesis offers a form of explanation grounded in observable evidence rather than black box model internals.

The third and final contribution of my thesis addresses one limitation of IRJIT: reliance on purely lexical tokens. Pure lexical similarity is often insufficient to find structurally similar snippets, as it ignores program structure that may be relevant to bug introduction. In this contribution, I examine whether richer structure based code representations can further improve SDP performance. Tree kernels provide a theoretically appealing way to capture syntactic structure through abstract syntax trees, and evaluating them allowed us to test whether structural expressiveness translates into practical benefit. 

The results shown in Chapter~\ref{chp:treekernels} show that tree kernels can capture structural similarity in code, providing a richer representation than purely lexical approaches. However, their computational cost is high due to quadratic comparison complexity which constrains their use. 
Due to the philosophy of realism adopted in this thesis, the high computational cost of tree kernels presents a challenge that had to be carefully considered. While structural information is valuable, expensive models must be justified by clear empirical gains.

%A reasonable deployment strategy is therefore hierarchical: inexpensive, fast methods first, followed by more expressive techniques only when warranted
Therefore, in the third contribution of this thesis, I propose a hybrid approach that first applies the inexpensive retrieval mechanism from IRJIT, followed by tree kernels. 
Theoretically, this strategy was a reasonable candidate for empirical evaluation, as it reflects the thesis's emphasis on practicality and scalability in model design. 

Taken together, these contributions suggest that several widely accepted conclusions in software defect prediction are artifacts of unrealistic evaluation and execution assumptions. This thesis does not claim that existing SDP research is invalid, but rather that such conclusions may be questionable and provide an incomplete picture. By introducing time-aware evaluation, incrementally updatable models, and evaluating structural representations, this work reframes how defect prediction models should be evaluated, and designed for realistic settings.


%%By introducing time-aware evaluation, incrementally updatable models, and evaluating structural representations, this work provides evidence that challenges how defect prediction models are currently assessed and motivates more realistic evaluation and design choices.
%%SDP research needs to account for operational constraints, not just accuracy.

%\section{Choice of IR and Tree Kernels instead of ML and DL}
%Most studies use ML, we use IR. Why? How does IR benefit us in designing a realistic approach?
%Light weight, up-to-date/online models, fast to update models, explain-ability, granular predictions.


\section{Design Choices and Limitations}
In this section we reflect on different choices made throughout this thesis and how it impacted the outcomes.

% how to extend explainability and how to evaluate it

%Did we use the right metrics to evaluate IRJIT}

%1. Lexical models are not aware of the contextual semantic information. code frequencies do not care about code semantics. IRJIT overlooks token semantics e.g. it does not understand that 'for' is similar to 'while'.
%2. High FPs for some projects.
%3. We did not evaluate its usefulness with real developers.
%
%
%
%How IRJIT is explainable ?
%
%Consider two illustrative instances: in one, the model solely yields a binary label denoting the presence of vulnerabilities in a code snippet, while in the other, it provides a label accompanied by relevant code snippets showcasing similar vulnerabilities. This latter approach facilitates a deeper understanding of the model’s decision through additional explanation. 
%
%In my thesis, industrial validation was not done and none of the proposed methods were evaluated in an industrial setting neither the approaches were deployed, but realism is substantially improved relative to prior work. The thesis thus outlines a path toward SDP methods that are not only empirically sound, but also credible candidates for adoption in real software development environments.
%
%
%In our research also, we have evaluated the explainability of the state-of-the-art models, but our approach has  limitations, such as the claims of explainability that are not empirically evaluated. We assumed transparent models like IRJIT are explainable, but it does not mean that they are actionable? There is no evaluation to find out if these models provide enough information for end-users to act upon and trust the outputs? Are all transparent models equal in terms of practitioners' ability to understand them? There is enormous potential for future research on the relationship between explainability and the usefulness to practitioners.
%
%
%
%Explainability includes post hoc explanations [116], offering supplemental insights to further clarify the model's decisions. However, researchers have identified inconsistency among explanations provided by different techniques, highlighting the need for more reliable methods.
%
%Another issue is that there is no standard way, benchmark datasetes, or metrics to measure explainability of SDP models.
%Providing fine-grained line-level, interpretable defect predictions increases practical usefulness and improve developer trust but we have not evaluated these claims.
%
%
%In future, we might consider measuring explanation satisfaction, which considers how well the user understands the explanation presented by the model [91]. Variable importance, for example, may not be sufficient for practitioners to understand the model. We could engineer better and more meaningful features for these models, including the potential causes, types, risks or impacts of a bug as variables in the machine-learning models.
%

\subsection{Information Retrieval and Limitations}

As an information retrieval based approach, IRJIT relies on lexical similarity and token frequency. While this choice enables designing lightweight, online models, it also means that the model is insensitive to deeper syntactic and semantic relationships in code. IRJIT builds a vocabulary incrementally as it sees new code. Anything it has not seen is out-of-vocabulary and therefore does not contribute to the predictions, which weakens its generalization beyond the exact tokens it has seen. In this regard, n-grams~\cite{hindle2012,santos2018syntax} are potentially a better choice as they cater to out-of-vocabulary problem through techniques like smoothing. 

Moreover, the token similarity based hints can be coincidental contributing to the high false positive rates as observed for some projects in Table~\ref{table:stats}. This behavior reflects a broader trade-off inherent in retrieval-based approaches: prioritizing recall and simplicity can come at the cost of precision in certain contexts. While this thesis reports these outcomes transparently, it does not propose mechanisms to systematically reduce false positives.%without sacrificing the model's online and incremental nature. 
Addressing this issues remains an open problem for future work.

The lack of structural and semantic similarity modeling limits the effectiveness of IRJIT. To partially address this, I incorporated structural similarity signals derived from tree kernels into the IRJIT model.
However, in its current form, IRJIT still does not capture functional similarity between constructs such as \textit{for} and \textit{while}, nor does it reason about control flow or behavioral equivalence. As a result, semantically similar changes may be treated as unrelated, which can limit prediction quality in SDP settings where semantic variation dominates over lexical similarity.


With respect to explainability, this thesis adopts reasonable assumptions but does not provide an empirical evaluation. IRJIT is explainable in the sense that it produces explanations in the form of documents that support the predictions such as similar past commits, and source code lines ranked by riskiness. However, this work does not empirically evaluate whether such explanations are useful for the developers or contain sufficient information for them to act upon. Arguably, this additional context may facilitate a better understanding of model behavior compared to models that output only a binary label, or rely on post-hoc explanations, which have been shown to produce inconsistent or even contradictory insights~\cite{roy2022don}.


That being said, it is imperative to validate such assumptions with the developers as transparency does not necessarily imply usefulness, and not all transparent models are equally understandable in practice. A major barrier in the evaluation of explainability is the substantial resources required to conduct user studies. 
Moreover, there are no standard methodologies, evaluation benchmarks, datasets, or metrics for measuring explainability in the area.
While this thesis discusses explainability and its potential approaches, it does not resolve these foundational measurement challenges. 


\subsection{Tree Kernels and Limitations}

Tree kernels offer an alternative to embeddings for capturing code similarity, and each approach has distinct trade-offs. Tree kernels work directly on ASTs, which are a natural representation of source code and are understandable by humans, unlike embeddings, which are hard to visualize.
In the context of SDP, tree kernels offer the attractiveness of a simple non-neural alternative, that requires no training or GPU resources, and computes similarity on-the-fly without defining explicit feature vectors. 
Consistent with this thesis's focus on simple, lightweight methods applicable in real-world settings with limited offline training budgets, these characteristics motivated the evaluation of tree kernels.
Additionally, the deterministic nature of tree kernels ensures that the same two ASTs will always yield identical similarity scores.


Embeddings, on the other hand, are derived from deep learning models and need significant offline training to produce vector representations. Once the training is done, similarity comparisons are fast. Effectively, they shift the computational cost from inference to the training phase. This shift is problematic in evolving systems characterized by changes in data distributions, concept drift, and verification latency. Without retraining, their representations may become misaligned with the latest code changes, which can lead to degraded performance in realistic, continuously evolving development environments. The use of embeddings or similar models therefore does not align with the thesis's goal of realistic incremental model development.


In my opinion, neither alternative is strictly superior; rather, they reflect different priorities. Tree kernels offer a training-free, deterministic similarity that is immediately applicable to new data. In this thesis, they served as an underexplored, practical alternative to embedding-based similarity. 
While they align well with the broader focus of this thesis on simple, incrementally usable models, but their high computational cost and slow nature at inference time (Chapter~\ref{chp:treekernels}) limits their practical value for code search and retrieval. In contrast, embeddings are usually pre-computed, which moves their training and feature construction overhead offline to enable fast runtime comparisons. %This motivated the hybrid design in the third contribution where IRJIT's fast retrieval was combined with tree kernel re-ranking, to empirically assess the benefits of structural richness while maintaining practical efficiency.

%Why did tree kernels not perform as expected. They should have been evaluated in a supervised manner.
%Tree kernels can be used for program repair?

\subsection{Lack of Industrial Evaluation }

Another limitation of my thesis is the lack of its industrial validation. The hypothesis proposed in Chapter~\ref{chp:intro} and the relevant techniques were neither tested in actual software development settings nor assessed through user studies involving developers. Consequently, claims about practical usefulness, or adoption are necessarily indirect. 
That said, the thesis improves realism relative to much prior SDP research by adopting time-aware evaluation, incremental model updates, and execution constraints that more closely reflect operational settings. While these contributions point toward practical SDP methods, they do not constitute evidence of industrial uptake.

%Lessons from Tree Kernels

 
\section{Do Large Language Models make SDP irrelevant?}

Research conducted prior to the appearance of Large Language Models (LLMs) naturally invites the question of whether such work remains relevant in light of the recent advances LLMs have brought to SE. This concern is understandable given the impressive capabilities demonstrated by LLMs; however, we cannot overlook the fundamental limitations of LLM-based approaches. In this section, I argue that rather than making SDP obsolete, LLMs introduce new challenges that further reinforce the need for reliable, time-aware SDP models.

\subsection{Faster code production and increased need for SDP}
One immediate consequence of LLM adoption is the change in the scale at which code is produced, which has direct implications for defect prediction.
LLMs significantly increase the speed and volume of code production. 
As a result, the burden on code review, testing, and quality assurance has increased. As more code is produced in less time, the feasibility of exhaustive testing and manual review has decreased further. In such settings, it becomes critical to identify the risky commits and prioritize their further scrutiny. SDP models are explicitly designed for this purpose, as they can estimate the riskiness of a commit and alert developers to further test it. Therefore, LLMs do not reduce the need for SDP; their use strengthens the case for scalable, SDP techniques.


%\subsection{LLM code is not defect free} 
%SDP does not target syntax errors, as most commits are already free of them due to compilation and continuous integration checks; instead, it estimates defect risk using historical data. Similarly, LLM-written code is often syntactically correct and compilable, but empirical evidence shows that it still contains other defects~\cite{tambon2025bugs}, necessitating the continued use of SDP.
%
%The most prevalent defects in practice are logical, semantic, and contextual, arising from misunderstandings of requirements, missing corner cases, and complex interactions within evolving codebases. LLM-based commits may be more risky in certain contexts as the increased human reliance on LLMs can let such subtle defects to slip through the development and review processes. These could have been caught otherwise through developer scrutiny. Empirical evidence also suggests that LLM-generated code is particularly prone to such issues, including misunderstandings of intent, incorrect assumptions, and flawed logic~\cite{dou2026wrong}.
%
%
%Several studies further report concerns related to performance, maintainability, and security~\cite{tihanyi2025secure}. These findings indicate that LLMs shift the defect landscape rather than eliminating defects altogether. Consequently, the core problem that JIT-SDP addresses i.e., identifying risky changes before they cause failures, remains unresolved.

SDP targets non-syntactic defects, since commits are already largely free of syntax errors due to compilation and continuous integration checks. Instead, SDP estimates the risk of logical, semantic, and contextual defects using historical data.

LLM-written code follows the same pattern: it is often syntactically correct and compilable, yet empirical studies show that it still contains such defects~\cite{tambon2025bugs}. These defects arise from misunderstandings of intent, missing corner cases, and complex interactions in evolving codebases, and evidence suggests that LLM-generated code is particularly prone to these issues~\cite{dou2026wrong}.

Moreover, LLM-based commits may be more risky because increased reliance on LLMs may reduce human scrutiny during development and review, allowing subtle defects to pass undetected. Recent research reports concerns around performance, maintainability, and security of LLM-written code~\cite{tihanyi2025secure}. Together, these findings indicate that LLMs shift the defect landscape rather than eliminate those. Consequently, the core problem addressed by JIT-SDP i.e., identifying risky changes at check-in time, remains relevant in the era of LLMs.

\subsection{LLMs are non-deterministic}
%LLMs offer a plausible way to assess the riskiness of software commits by reasoning about the semantic content of code changes, which are difficult to capture using historical repository data alone. In contrast, traditional software defect prediction models rely on long-term project signals such as developer history and prior defect trends. These models produce reproducible, auditable, and explainable results. At the same time, LLMs operate as probabilistic inference systems, and their outputs are inherently non-deterministic, varying with prompt formulation, model updates, and deployment conditions, which limits their suitability for consistent quality assurance. 
%
%While LLMs have reasoning ability, and they can produce post-hoc explanations in natural language, their internal working is opaque and difficult to validate empirically. Together, these differences suggest that LLM-based and traditional SDP approaches focus on different dimensions of commit riskiness, and theoretically neither approach fully subsumes the other. Therefore, combining them into a single approach may yield more accurate, robust, and actionable predictions and warrants empirical investigation.


LLMs can be used for software defect prediction by leveraging pretrained semantic knowledge to assess code changes, capturing riskiness that may not be well represented in project specific historical data alone.
In contrast, traditional SDP models estimate risk using long-term project signals such as developer activity, process metrics, and prior defect trends, yielding reproducible, auditable, and explainable predictions.

However, LLMs function as probabilistic inference systems whose outputs may vary with prompt formulation, model updates, and deployment conditions, limiting their suitability for consistent and verifiable quality assurance. Although LLMs can generate natural-language explanations, their internal decision processes remain opaque and hard to validate empirically. These differences indicate that traditional vs LLM-based SDP approaches capture different dimensions of commit riskiness rather than the same signal. As a result, neither approach fully subsumes the other, and their combination may provide more robust and actionable defect risk estimates.

\subsection{Effectiveness of SDP on LLM written code}

Despite the widespread adoption of LLMs, there is a limited empirical evidence on how traditional SDP models perform in environments where some of the code is generated by LLMs.
There are no established datasets comparing defect distributions before and after LLM adoption, nor systematic studies examining whether SDP models trained on human-written code remain valid for evaluating the riskiness of LLM-generated code. This gap suggests the need to re-evaluate the existing SDP research considering these additional perspectives, using better datasets, and appropriate evaluation methodologies. 
 
This thesis contributes in this direction by examining how defects can be reliably predicted in evolving software systems. It challenges prevalent evaluation practices in SDP research and shows that conclusions drawn from static evaluation setups are often unstable over time. By demonstrating temporal instability in defect prediction results and exploring simple incremental  approaches, this work highlights the importance of temporally stable evaluations and opens new directions for future SDP research.

\subsection{Do not include}
Is SDP still needed now that LLM's have taken over?
LLMs are probabilistic inference not deterministic.

Have defects increased or decreased with the emergence of LLMs? Does defect distribution change after LLMs? Does the nature and type of defects change? There are papers that have evaluated if LLMs increase or decrease bugs. \\
 
Did the defect distribution change?
Clarify if your data has syntax errors? Report the taxonomy of bugs in historical data and in LLM code?

LLMs are great at producing syntax free code. However, commits are usually syntax free and they contain logical or semantic bugs that LLMs may overlook. Therefore, SDP models are still relevant today.
Software projects often have unique, evolving errors specific to individual code repositories.  
 
Software is part of all domains, including healthcare and security-critical, where the consequences  of bugs are more severe. This has increased the need of reliable code even more. Clankers are producing code at a very fast rate so the need to code review has increased as well. Even code written by LLMs can be bug prone and people care about the riskiness of commits. Due to these reasons the need of SDP models is high but SDP's effectiveness has not improved. With the amount of code we are producing that needs review, the slow models may not remain very useful. Models need to adapt fast. A model trained a year ago may not hold its performance as underlying assumptions could change. Especially with LLMs the old models become obsolete fast.
 
While we may use LLMs to predict bug proneness of code but they may produce many false positives. Therefore, SDP in combination with LLMs might be a much reliable option.
 
Therefore, reliable defect predictions models are still of importance. They should scale to the large amounts of code without being too slow.
 
 
SDP as a field has not kept up with the development pace of SE where LLMs have taken over.
The whole SDP research is questionable and it needs to be re-evaluated. We need to establish strong datasets, better baselines, and an agreed upon evaluation criteria.
 
%LLMs have reasoning ability so we can produce reasonable estimates of code coming in.
%See A reflection on change classification in the era of LLMs.
 
LLMs may also get affected by time-Based Output Drift. Furthermore, there is no assurance that the results will remain consistent over time.
 
AI can automatically generate tests for new code, even without history.
Running these tests gives immediate signals: fail → likely buggy, pass → likely okay.
This could augment SDP models, especially in early stages.
 
LLM-generated tests are not perfect. They might miss edge cases, misunderstand the code, or give false positives/negatives. Traditional SDP can combine multiple sources of information (history, code metrics, developer patterns) to make more reliable predictions. In practice, the best approach is hybrid: use AI-generated test outputs as additional features for SDP, rather than replacing it entirely.

Unlike LLMs, our approach does not rely on training a model with hundreds of billion parameters or generalizing across datasets. It directly analyses the raw data from repositories (code and its structure), making it inherently reproducible and free from probabilistic biases. LLMs are not deterministic.

Their resource requirements are significantly higher, both in terms of model size, training time and computational power. 
There is differences in the cost of repairing buggy Copilot-generated code compared to human-generated code.

A study found that although Copilot improves productivity, it often produces code with insecure patterns. (An Empirical Cybersecurity Evaluation of GitHub Copilot’s Code Contributions)

Several studies point out that LLMs negatively impact software development from a security perspective.
Our study reveals that at least 62.07\% of the LLM generated programs have security vulnerabilities. 
%"How secure is AI-generated code: a large-scale comparison of large language models"

Kla found four types of errors in ChatGPT-generated code: compilation and runtime errors, wrong output (linked to assertion errors), code style and maintainability, and performance and efficiency. ("Refining chatgpt-generated code: characterizing and mitigating code quality issues")

There are no specific studies comparing types or distribution of bugs before and after the adoption of LLMs.

No studies on assessing the effectiveness of traditional SDP models (that are trained solely on human-written code) on the software systems where part of the code is written by AI.


LLMs Speeds up development.
LLM generated code can be syntactically correct but semantically buggy. Developers still need to review carefully.

%"Bugs in large language models generated code: an empirical study"
10 distinctive bug patterns bug patterns were found in code generated by LLMs. These include Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, and Non-Prompted Consideration. 

%"What is wrong with your code generated by large language models? An extensive study"
This paper developed a taxonomy of bugs for LLM code that includes three categories (functional, syntax and runtime bugs) and ten sub-categories, and analyzed the root cause for common bug types. They found that functional bugs and misunderstanding and logic errors are the highest percentage of bugs produced by LLMs.

\section{Threats to Validity}
\subsection{Low Quality SDP Datasets} 
This can briefly be discussed in threats to validity section as it is not a limitation of my work but SDP in general.

The only defects that get noticed are the ones that make it to the issue tracker and that's what SDP addresses.

SZZ and its variants do not do a great job at labeling. Explainability datasets and metrics are not available.

We operate on the operational data.
We are not totally aware of all the issues and bugs.
We operate on the data in the wild.
No instrumented product where we are aware of all issues.
No solely benchmark tool.
Make evaluation benchmark.

%See A Reflection on Change Classification in the Era of Large Language Model

%The repair datasets might give a perspective.

%Daniel Alencar da Costa's survey has some great pointers on which studies offer explainability.

%(see /home/hareem/ThinkpadP14/UofA2023/Tree_Kernel2024/NotesWinter2024.md)


\end{document}

%We only use code but other sources of information generated during software development may be just as important as the code.


%Bugs negatively impact a system particularly in safety and security-critical domains like autonomous driving (Chen et al., 2015) and healthcare (Cai et al., 2014), the consequences of these bugs can be more severe. 

%Parse tree kernels have been successfully used in natural language processing (NLP). However, the parse tree kernel used in NLP does not perform well for program source code due to two issues. The first issue is the asymmetric influence of node changes. In previous tree kernels, changes near a root node have larger influence than changes near leaf nodes. When the tree depth is small, this effect is not serious. However, the parse trees of program sources are much larger than that of natural language sentences, and this unwanted influence greatly affects tree comparison. The second issue is the sequence of subtrees. Previous tree kernels count sequence of subtrees. Unlike natural language sentences, the sequence of two substructures (like the order of two methods in a Java class) has little information in program source codes. We identified these two issues, and propose a new parse tree kernel for program source code.


%When processing ASTs structures, we usually face a problem of high-dimensional data. The size of an AST increases more than linearly when the program size is larger. In the experimental dataset, the largest AST contains 7027 nodes, while the program has only 343 lines of code including comments and blank lines. High-dimensional data not only lead to waste of time and memory but also aﬀect the performance of algorithms. Thus, dimension reduction is an essential task for the AST-based approach.
